<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="description" content=""> <meta property="og:title" content=""> <meta property="og:description" content=""> <meta property="og:url" content=""> <meta property="og:image" content=""> <meta property="og:image:width" content=""> <meta property="og:image:height" content=""> <meta name="twitter:title" content=""> <meta name="twitter:description" content="Are CLIP features all you need for Universal Synthetic Image Origin Attribution?"> <meta name="twitter:card" content="summary_large_image"> <meta name="keywords" content="Open Set Origin Attribution,Diffusion Models,Deepfake Detection,Open Set Recognition"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>Are CLIP features all you need for Universal Synthetic Image Origin Attribution?</title> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="static/css/bulma.min.css"> <link rel="stylesheet" href="static/css/bulma-carousel.min.css"> <link rel="stylesheet" href="static/css/bulma-slider.min.css"> <link rel="stylesheet" href="static/css/fontawesome.all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel="stylesheet" href="static/css/index.css"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script> <script defer src="static/js/fontawesome.all.min.js"></script> <script src="static/js/bulma-carousel.min.js"></script> <script src="static/js/bulma-slider.min.js"></script> <script src="static/js/index.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body, );"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})});</script> </head> <body> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title">Are CLIP features all you need for Universal Synthetic Image Origin Attribution?</h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://ciodar.github.io" target="_blank">Dario Cioni</a><sup>1, 3</sup>,</span> <span class="author-block"> <a href="https://chi0tzp.github.io" target="_blank" rel="external nofollow noopener">Christos Tzelepis</a><sup>2</sup>,</span> <span class="author-block"> <a href="http://www.micc.unifi.it/seidenari" target="_blank" rel="external nofollow noopener">Lorenzo Seidenari</a><sup>1</sup>,</span> <span class="author-block"> <a href="https://www.eecs.qmul.ac.uk/~ioannisp/" target="_blank" rel="external nofollow noopener">Ioannis Patras</a><sup>3</sup> </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"><sup>1</sup>University of Florence, <sup>2</sup>City, University of London, <sup>3</sup>Queen Mary, University of London<br>TWYN @ ECCV 2024</span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://github.com/ciodar/UniversalAttribution" target="_blank" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> <span class="link-block"> <a href="https://arxiv.org/abs/2408.09153" target="_blank" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="hero teaser"> <div class="container is-max-desktop"> <div class="hero-body"> <img class="is-centered" src="figures/overview.svg" alt="overview" width="100%"> <br> <h2 class="subtitle has-text-centered"> We propose to use general-purpose features extracted from large pre-trained vision encoders to perform Open-Set Origin Attribution of synthetic images produced by various generative models, including Diffusion Models. Our method outperforms existing frequency-based forensic classifiers, is able to operate in the low-data regime, and is more robust to input perturbations. </h2> </div> </div> </section> <section class="section hero is-light"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2 class="title is-3">Abstract</h2> <div class="content has-text-justified"> <p> The steady improvement of Diffusion Models for visual synthesis has given rise to many new and interesting use cases of synthetic images but also has raised concerns about their potential abuse, which poses significant societal threats. To address this, fake images need to be detected and attributed to their source model, and given the frequent release of new generators, realistic applications need to consider an Open-Set scenario where some models are unseen at training time. Existing forensic techniques are either limited to Closed-Set settings or to GAN-generated images, relying on fragile frequency-based "fingerprint" features. By contrast, we propose a simple yet effective framework that incorporates features from large pre-trained foundation models to perform Open-Set origin attribution of synthetic images produced by various generative models, including Diffusion Models. We show that our method leads to remarkable attribution performance, even in the low-data regime, exceeding the performance of existing methods and generalizes better on images obtained from a diverse set of architectures. </p> </div> </div> </div> </div> </section> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2 class="title">Method</h2> <div class="content has-text-justified"> <p> We address the problem of synthetic image attribution in the most general setting possible: <br> Using real images (from a set $\mathcal{R}$), synthetic images generated by a set of known generative models $\mathcal{O}_\mathcal{K}=\{\mathcal{M}_\mathcal{K}^1,\ldots,\mathcal{M}_\mathcal{K}^{N_{\mathcal{O}_\mathcal{K}}}\}$, and synthetic images generated by a set of unknown generative models $\mathcal{O}_\mathcal{U}=\{\mathcal{M}_\mathcal{U}^1,\ldots,\mathcal{M}_\mathcal{U}^{N_{\mathcal{O}_\mathcal{U}}}\}$, we optimize a classifier to assign images to either a known model from $\mathcal{O}_\mathcal{K}$ or ``reject'' such assignment, classifying the images as <i>synthetic-and-unknown</i> ($y_u$). </p> <p> Motivated by the generality and expressiveness of the representations of modern vision foundation models, we propose to employ the Vision Transformer-based encoder of a foundation model and extract intermediate features. Next, we perform the classification task (of assigning each image to a class in $\mathcal{O}_\mathcal{K}\cup\{y_u\}$) following either a Linear Probe or a $k$-NN approach. </p> </div> <div> <img src="figures/learning_methods.svg" class="is-centered" style="width: 90%"> </div> <br> <div class="content has-text-justified"> <p> </p> </div> </div> </div> <section class="section" id="Results"> <div class="container is-max-desktop content"> <h2 class="title">Results</h2> <h3 class="title">Open Set Attribution</h3> <img class="is-centered" src="figures/genimage-comparison.png" alt="overview" width="100%"> <h3 class="title">Architecture ablation</h3> <img class="is-centered" src="figures/architecture-sweep.png" alt="overview" width="100%"> <h3 class="title">Robustness</h3> <img class="is-centered" src="figures/robustness.png" alt="overview" width="100%"> </div> <div class="container is-max-desktop content"> </div> </section> <section class="section" id="BibTeX"> <div class="container is-max-desktop content"> <h2 class="title">BibTeX</h2> <pre><code>@misc{cioni2024clip,
        title={Are CLIP features all you need for Universal Synthetic Image Origin Attribution?},
        author={Dario Cioni and Christos Tzelepis and Lorenzo Seidenari and Ioannis Patras},
        year={2024},
        eprint={2408.09153},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
    }</code></pre> </div> </section> <footer class="footer"> <div class="container"> <div class="columns is-centered"> <div class="column is-8"> <div class="content"> <p> This research was supported by the EU's Horizon 2020 programme H2020-951911 <a href="https://www.ai4media.eu/" rel="external nofollow noopener" target="_blank">AI4Media</a> project. </p> <p> This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="external nofollow noopener">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank" rel="external nofollow noopener">Nerfies</a> project page. We thank the authors for sharing the templates. </p> </div> </div> </div> </div> </footer> </body> </html>